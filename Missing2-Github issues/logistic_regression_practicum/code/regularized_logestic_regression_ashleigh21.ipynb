{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gradient_descent_solutionnnnnt import GradientDescent\n",
    "\n",
    "__author__ = \"Jared Thompson\"\n",
    "\n",
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, fit_intercept = True, scale = True, norm = \"L2\"):\n",
    "        '''\n",
    "        INPUT: GradientDescent, function, function, function\n",
    "        OUTPUT: None\n",
    "        Initialize class variables. Takes three functions:\n",
    "        cost: the cost function to be minimized\n",
    "        gradient: function to calculate the gradient of the cost function\n",
    "        predict: function to calculate the predicted values (0 or 1) for\n",
    "        the given data\n",
    "        '''\n",
    "        \n",
    "        # * The use of some of these attributes are a matter of taste. \n",
    "        # You're going to want to store user selection of norm and fit_intercept.\n",
    "        \n",
    "        # Note that the cost_gradients are given below\n",
    "        gradient_choices = {None: self.cost_gradient, \"L1\": self.cost_gradient_lasso, \"L2\": self.cost_gradient_ridge}\n",
    "\n",
    "        self.alpha = None\n",
    "        self.gamma = None\n",
    "        self.coeffs = None\n",
    "        self.num_iterations = 0\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.scale = scale\n",
    "        self.normalize = False\n",
    "        if norm:\n",
    "            self.norm = norm\n",
    "            self.normalize = True\n",
    "        # All of the magic of regularization is done here. \n",
    "        # You pass this choice of function into the gradient descent as \n",
    "        # self.gradient\n",
    "        self.gradient = gradient_choices[norm] \n",
    "        self.mu=0\n",
    "        self.sigma=1\n",
    "        #fit_intercept=True, normalize=False, gradient=None, mu=None, sigma=None\n",
    "#        self.threshold=threshold\n",
    "    def fit(self,  X, y, alpha=0.01, num_iterations=10000, gamma=0.):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, float, int, float\n",
    "        OUTPUT: numpy array\n",
    "        Main routine to train the model coefficients to the data\n",
    "        the given coefficients.\n",
    "        '''\n",
    "        # * just as you did in gradient_descent, you'll need to store\n",
    "        # the alpha and gamma variables and the \n",
    "        # number of interations you intend to run.\n",
    "        self.alpha=alpha\n",
    "        self.gamma=gamma\n",
    "        self.num_iterations=num_iterations\n",
    "        # You may also consider storing the dimensions of X.\n",
    "        self.dimensions=X.shape\n",
    "        \n",
    "        \n",
    "        # * You need to initialize the coefficients - I recommend random numbers.\n",
    "        # It is at this point that you'll be determining the data structure of          \n",
    "        # the coefficients. I recommend numpy vectors.\n",
    "        self.coeffs = np.random.random(X.shape[1])\n",
    "                \n",
    "        # * Instantiate a gradientdescent instance\n",
    "        gradient_descentInstance=GradientDescent(self.fit_intercept, self.normalize, self.gradient)\n",
    "        # * Now .run() the gradient. Notice that you need to use the local copy        \n",
    "        # of coefficients as an argument to .run(). The gradientdescent instance       \n",
    "        # calculates and stores the new coefficients to itself, i.e.\n",
    "        # gradient.coeffs\n",
    "        inst=gradient_descentInstance.run(X,y,self.coeffs)\n",
    "        \n",
    "        # * save the newly calculated coefficients to self.coeffs.\n",
    "        self.coeffs=inst.coeffs\n",
    "        return gradient_descentInstance.newX\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array\n",
    "        OUTPUT: numpy array\n",
    "        Calculate the predicted values (0 or 1) for the given data with\n",
    "        the given coefficients.\n",
    "        '''\n",
    "        # * You need to think of this as returning the value of the hypothesis          \n",
    "        # function, but rounded using a threshold. One easy way to do this is to        \n",
    "        # assume that the threshold is 0.5 and just use the np.around() function.         \n",
    "        # Return a bool or binary valued array, one finding for each data point in X.\n",
    "        \n",
    "        if self.normalize:\n",
    "            X = (X - self.mu) / self.sigma\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            X = np.hstack((np.ones((X.shape[0],1)), X))\n",
    "        \n",
    "        number=self.hypothesis(X, self.coeffs)\n",
    "        print(number)\n",
    "        n=np.around(number)\n",
    "        return n\n",
    "\n",
    "    def hypothesis(self, X, coeffs):\n",
    "        '''\n",
    "        THIS IS WHERE I HAD THE MOST ISSUES\n",
    "        \n",
    "        \n",
    "        INPUT: 2 dimensional numpy array, numpy array\n",
    "        OUTPUT: numpy array of floats\n",
    "        Calculate the predicted percentages (floats between 0 and 1)\n",
    "        for the given data with the given coefficients.\n",
    "        '''\n",
    "        # * Here you're returning a vector of scalar value calculated from the \n",
    "        # hypothesis function h(X) = 1/1+e^{-BX}. Remember that each row of the \n",
    "        # feature matrix is a set of feature values for a single datapoint. The        \n",
    "        # hypothesis function requires a dot product between the coefficient      \n",
    "        # vector and each of those values to get a single estimate for that data     \n",
    "        # point.\n",
    "        if X.shape[1] != len(coeffs):\n",
    "            print(\"AHHHHHH\")\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(- (X.dot(coeffs))))\n",
    "\n",
    "    def cost_function(self, X, y, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, numpy array\n",
    "        OUTPUT: float (a scalar)\n",
    "        Calculate the value of the cost function for the data with the\n",
    "        given coefficients.\n",
    "        '''\n",
    "        # * you'll need to have the vector of values from the hypothesis function\n",
    "        hypoth_vector = self.hypothesis(X, coeffs)\n",
    "        \n",
    "        y_left = y.T.dot(log(hypoth_vector))\n",
    "        y_right = (1 - y).T.dot(np.log(1 - hypoth_vector))\n",
    "        m=y.shape[0]\n",
    "        \n",
    "        cost=(1/m)*((y_right)+(y_left))\n",
    "        return cost\n",
    "        \n",
    "\n",
    "        # * Return the cost function, 1/m*(y.h(x)+(1-y).(1-h(x)))\n",
    "\n",
    "    def cost_lasso(self, X, y, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, numpy array\n",
    "        OUTPUT: float\n",
    "        Calculate the value of the cost function with lasso regularization\n",
    "        for the data with the given coefficients.\n",
    "        '''\n",
    "        # * you'll need to have the vector of values from the hypothesis function\n",
    "\n",
    "        # * you'll return the cost function as above plus the appropriate formula\n",
    "        # for the regularization\n",
    "        cost=self.cost_function(X, y, coeffs)\n",
    "        lasso=self.gamma * sum(abs(coeffs))\n",
    "        cost_lass=cost+lasso\n",
    "        return cost_lass\n",
    "\n",
    "    def cost_ridge(self, X, y, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, numpy array\n",
    "        OUTPUT: float\n",
    "        Calculate the value of the cost function with ridge regularization\n",
    "        for the data with the given coefficients.\n",
    "        '''\n",
    "        # * you'll need to have the vector of values from the hypothesis function\n",
    "\n",
    "        # * you'll return the cost function as above plus the appropriate formula\n",
    "        # for the regularization\n",
    "        cost = self.cost_function(X, y, coeffs)\n",
    "        ridge = self.gamma * coeffs.dot(coeffs.T) / 2\n",
    "        \n",
    "        cost_ridgee=cost+ridge\n",
    "        return cost_ridgee\n",
    "\n",
    "    def cost_gradient(self, X, y, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, numpy array\n",
    "        OUTPUT: numpy array\n",
    "        Calculate the gradient of the cost function at the given value\n",
    "        for the coeffs.\n",
    "        Return an array of the same size as the coeffs array.\n",
    "        '''\n",
    "        # * you'll need to have the vector of values from the hypothesis function\n",
    "        # * return the formula that you learned in class X^T.(y-h)\n",
    "        hyp = self.hypothesis(X, coeffs)\n",
    "        cost_grad=X.T.dot(y - hyp)\n",
    "        return cost_grad\n",
    "\n",
    "    \n",
    "    def cost_gradient_lasso(self, X, y, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, numpy array\n",
    "        OUTPUT: numpy array\n",
    "        Calculate the gradient of the cost function with regularization\n",
    "        at the given value for the coeffs.\n",
    "        Return an array of the same size as the coeffs array.\n",
    "        '''\n",
    "        # * you'll need to have the vector of values from the hypothesis function\n",
    "        # * you will also need to calculate the appropriate vector of \n",
    "        # coefficients for regularizationn (weights). Do the calculus yourself.\n",
    "\n",
    "        # * make sure to insert an intercept\n",
    "\n",
    "        # * return the formula above plus gamma*weights/n\n",
    "        shape= y.shape[0]\n",
    "        cost_grad=self.cost_gradient(X, y, coeffs)\n",
    "        other = self.gamma * (np.sum(np.absolute(coeffs))) / (2 * shape)\n",
    "        return cost_grad+other\n",
    "        \n",
    "\n",
    "    def cost_gradient_ridge(self, X, y, coeffs):\n",
    "        '''\n",
    "        INPUT: 2 dimensional numpy array, numpy array, numpy array\n",
    "        OUTPUT: numpy array\n",
    "        Calculate the gradient of the cost function with regularization\n",
    "        at the given value for the coeffs.\n",
    "        Return an array of the same size as the coeffs array.\n",
    "        '''\n",
    "        # * you'll need to have the vector of values from the hypothesis function\n",
    "        hypoth_vector = self.hypothesis(X, coeffs)\n",
    "                                                              \n",
    "        shape= y.shape[0]\n",
    "        cost_grad=self.cost_gradient(X, y, coeffs)\n",
    "        other = self.gamma * (np.sum(coeffs**2)) / (2 * shape)\n",
    "        return cost_grad+other\n",
    "        # * you will also need to calculate the appropriate vector of \n",
    "        # coefficients for regularization(weights). Review the notes to see how          \n",
    "        # simple this is.       \n",
    "\n",
    "        # * make sure to insert an intercept\n",
    "        \n",
    "        # * return the formula above plus gamma*weights/n\n",
    "                                                              \n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('../data/grad.csv', delimiter=',')\n",
    "y = data[:, -1]\n",
    "X = data[:, 0:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (4,1) doesn't match the broadcast shape (4,401)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e16c95be19ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtester\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-6e6e1a4aef4d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, alpha, num_iterations, gamma)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# calculates and stores the new coefficients to itself, i.e.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# gradient.coeffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0minst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_descentInstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# * save the newly calculated coefficients to self.coeffs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ashleighdiamond/Desktop/github/Missing2-Github issues/logistic_regression_practicum/code/gradient_descent_solutionnnnnt.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, X, y, coeffs, alpha, num_iterations)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# over a list and update each coefficient individually.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m-=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;31m#      return self.coeffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (4,1) doesn't match the broadcast shape (4,401)"
     ]
    }
   ],
   "source": [
    "tester=LogisticRegression()\n",
    "tester.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashleighdiamond/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#fit_intercept, threshold, num_iterations, alpha, reglambda, print_details = True, 0.5, 10000, 0.01, 1, False\n",
    "data = pd.read_csv(\"../data/grad.csv\")\n",
    "X = data[['gre', 'gpa', 'rank']].values\n",
    "y = data['admit'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (4,1) doesn't match the broadcast shape (4,300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0ab9403fce5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtester\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-6e6e1a4aef4d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, alpha, num_iterations, gamma)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# calculates and stores the new coefficients to itself, i.e.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# gradient.coeffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0minst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_descentInstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# * save the newly calculated coefficients to self.coeffs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ashleighdiamond/Desktop/github/Missing2-Github issues/logistic_regression_practicum/code/gradient_descent_solutionnnnnt.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, X, y, coeffs, alpha, num_iterations)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# over a list and update each coefficient individually.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m-=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;31m#      return self.coeffs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (4,1) doesn't match the broadcast shape (4,300)"
     ]
    }
   ],
   "source": [
    "tester=LogisticRegression()\n",
    "tester.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = tester.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
